{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49f3ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import Package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re, unicodedata, string, typing, time ,os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3a5d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory C:\\Users\\taiti\\OneDrive\\MasterDegree\\BADS9000_IS\\ThaiDrQA\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/taiti/OneDrive/MasterDegree/BADS9000_IS/ThaiDrQA')\n",
    "print(f\"Current working directory {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5ecee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(\"Length of data: \", len(data['data']))\n",
    "    print(\"Data Keys: \", data['data'][0].keys())   \n",
    "    return data\n",
    "\n",
    "def load_article(file):\n",
    "    try :\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            context = f.read()\n",
    "    except :\n",
    "        context = \"-\"\n",
    "        print(f\"Error Article id {id}\")\n",
    "    return  context\n",
    "\n",
    "def gather_text_for_vocab(dfs:list):\n",
    "    '''\n",
    "    Gathers text from contexts and questions to build a vocabulary.\n",
    "    '''\n",
    "    text = []\n",
    "    total = 0\n",
    "    for df in dfs:\n",
    "        unique_contexts = list(df.context.unique())\n",
    "        unique_questions = list(df.question.unique())\n",
    "        total += df.context.nunique() + df.question.nunique()\n",
    "        text.extend(unique_contexts + unique_questions)\n",
    "    \n",
    "    assert len(text) == total #For debugging code\n",
    "    print(\"Number of sentences in dataset: \", len(vocab_text))\n",
    "    return text\n",
    "\n",
    "def build_word_vocab(vocab_text):\n",
    "    \"\"\"Tokenizer Ref : https://github.com/PyThaiNLP/pythainlp \"\"\"\n",
    "    words = []\n",
    "    for sent in vocab_text:\n",
    "        for word in word_tokenize(sent ,engine='newmm'):\n",
    "            words.append(word)\n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word, word_vocab\n",
    "\n",
    "def word_to_ids(text, word2idx):\n",
    "    '''\n",
    "    Converts word text to their respective ids by mapping each word\n",
    "    using word2idx. Input text is tokenized using spacy tokenizer first.\n",
    "    :param str text: context text to be converted\n",
    "    :param dict word2idx: word to id mapping\n",
    "    :returns list context_ids: list of mapped ids\n",
    "    :raises assertion error: sanity check\n",
    "    '''\n",
    "    words_tokens = [w for w in word_tokenize(text ,engine='newmm') ]\n",
    "    words_ids = [word2idx[word] for word in words_tokens]\n",
    "    \n",
    "    assert len(words_ids) == len(words_tokens)\n",
    "    return words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9838ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  4000\n",
      "Data Keys:  dict_keys(['question_id', 'question', 'answer', 'answer_begin_position ', 'answer_end_position', 'article_id'])\n"
     ]
    }
   ],
   "source": [
    "#Open Json File\n",
    "data = load_json('./data/ThaiQACorpus-DevelopmentDataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1474f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shap:(4000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115035</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;doc id=\"115035\" url=\"https://th.wikipedia.org...</td>\n",
       "      <td>สุนัขตัวแรกรับบทเป็นเบนจี้ในภาพยนตร์เรื่อง Ben...</td>\n",
       "      <td>[529, 538]</td>\n",
       "      <td>ฮิกกิ้นส์</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376583</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;doc id=\"376583\" url=\"https://th.wikipedia.org...</td>\n",
       "      <td>ลูนา 1 เป็นยานอวกาศลำแรกในโครงการลูนาของโซเวีย...</td>\n",
       "      <td>[139, 144]</td>\n",
       "      <td>เมชตา</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  id                                            context  \\\n",
       "0      115035   1  <doc id=\"115035\" url=\"https://th.wikipedia.org...   \n",
       "1      376583   2  <doc id=\"376583\" url=\"https://th.wikipedia.org...   \n",
       "\n",
       "                                            question       label     answer  \n",
       "0  สุนัขตัวแรกรับบทเป็นเบนจี้ในภาพยนตร์เรื่อง Ben...  [529, 538]  ฮิกกิ้นส์  \n",
       "1  ลูนา 1 เป็นยานอวกาศลำแรกในโครงการลูนาของโซเวีย...  [139, 144]      เมชตา  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load data JSON to DataFrame\n",
    "qa_lst = []\n",
    "for qa in data['data']:\n",
    "    qa_dict = {}\n",
    "    qa_dict['article_id'] = qa['article_id']\n",
    "    qa_dict['id'] = qa['question_id']\n",
    "    qa_dict['context']  = load_article( f\"./data/documents-nsc/{qa['article_id']}.txt\")\n",
    "    qa_dict['question'] = qa['question']\n",
    "    qa_dict['label'] = [qa['answer_begin_position '],qa['answer_end_position']]\n",
    "    qa_dict['answer'] = qa['answer']\n",
    "    qa_lst.append(qa_dict)\n",
    "\n",
    "df_qa = pd.DataFrame(qa_lst)\n",
    "print(f\"Shap:{df_qa.shape}\")\n",
    "display(df_qa.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "626df805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data \n",
    "df_qa.to_pickle('C:/Users/taiti/OneDrive/MasterDegree/BADS9000_IS/ThaiDrQA/data/df_thaiqa.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0c1364b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in dataset:  6266\n",
      "Wall time: 192 ms\n",
      "raw-vocab: 63648\n",
      "vocab-length: 63648\n",
      "word2idx-length: 63648\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "# gather text to build vocabularies\n",
    "%time vocab_text = gather_text_for_vocab([df_qa])\n",
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "751357ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.3 s\n",
      "Wall time: 535 ms\n"
     ]
    }
   ],
   "source": [
    "# numericalize context and questions\n",
    "%time df_qa['context_ids']   = df_qa.context.apply(word_to_ids,   word2idx=word2idx)\n",
    "%time df_qa['question_ids'] = df_qa.question.apply(word_to_ids,  word2idx=word2idx)\n",
    "df_qa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cdad3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(df, idx2word):\n",
    "    '''\n",
    "    Performs the tests mentioned above. This method also gets the start and end of the answers\n",
    "    with respect to the context_ids for each example.\n",
    "    :param dataframe df: SQUAD df\n",
    "    :param dict idx2word: inverse mapping of token ids to words\n",
    "    :returns\n",
    "        list start_value_error: example idx where the start idx is not found in the start spans\n",
    "                                of the text\n",
    "        list end_value_error: example idx where the end idx is not found in the end spans\n",
    "                              of the text\n",
    "        list assert_error: examples that fail assertion errors. A majority are due to the above errors\n",
    "    '''\n",
    "\n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "        answer_tokens = [w for w in word_tokenize(row['answer'] ,engine='newmm')]\n",
    "        \n",
    "        context_tokens = word_tokenize(row['context'] ,engine='newmm')\n",
    "        context_span  = [(len(\"\".join(context_tokens[0:i])), len(\"\".join(context_tokens[0:i+1]))) \n",
    "                         for i,w in enumerate(context_tokens)]\n",
    "        starts, ends = zip(*context_span)\n",
    "\n",
    "        answer_start, answer_end = (row['label'][0]-1,row['label'][1]-1)\n",
    "\n",
    "        try:\n",
    "            start_idx = starts.index(answer_start)\n",
    "        except:\n",
    "            start_value_error.append(index)\n",
    "        try:\n",
    "            end_idx  = ends.index(answer_end)\n",
    "        except:\n",
    "            end_value_error.append(index)\n",
    "\n",
    "        try:\n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\n",
    "        except:\n",
    "            assert_error.append(index)\n",
    "\n",
    "\n",
    "    return start_value_error, end_value_error, assert_error\n",
    "\n",
    "def get_error_indices(df, idx2word):\n",
    "    \n",
    "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f\"Number of error indices: {len(err_idx)}\")\n",
    "    \n",
    "    return err_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "44392cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 149\n",
      "Wall time: 13min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_err = get_error_indices(df_qa, idx2word)\n",
    "\n",
    "df_qa.drop(train_err, inplace=True)\n",
    "print(f\"Shape of data frame after drop error row: {df_qa.shape}\")\n",
    "#Some row is error explore that how to fixed that#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ba9dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_answer(row, idx2word):\n",
    "    '''\n",
    "    Takes in a row of the dataframe or one training example and\n",
    "    returns a tuple of start and end positions of answer by calculating \n",
    "    spans.\n",
    "    '''\n",
    "    context_tokens = word_tokenize(row['context'] ,engine='newmm')\n",
    "    context_span  = [(len(\"\".join(context_tokens[0:i])), len(\"\".join(context_tokens[0:i+1]))) \n",
    "                     for i,w in enumerate(context_tokens)]\n",
    "    starts, ends = zip(*context_span)\n",
    "    \n",
    "    answer_start, answer_end = (row['label'][0]-1,row['label'][1]-1)\n",
    "    \n",
    "    start_idx = starts.index(answer_start)\n",
    "    end_idx  = ends.index(answer_end)\n",
    "    \n",
    "    answer_tokens = [w for w in word_tokenize(row['answer'] ,engine='newmm')]\n",
    "    assert idx2word[row.context_ids[start_idx]] == answer_tokens[0]\n",
    "    assert idx2word[row.context_ids[end_idx]] == answer_tokens[-1]\n",
    "    \n",
    "    return [start_idx, end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "203f5a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape:(3851, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115035</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;doc id=\"115035\" url=\"https://th.wikipedia.org...</td>\n",
       "      <td>สุนัขตัวแรกรับบทเป็นเบนจี้ในภาพยนตร์เรื่อง Ben...</td>\n",
       "      <td>[529, 538]</td>\n",
       "      <td>ฮิกกิ้นส์</td>\n",
       "      <td>[101, 43, 0, 99, 27, 26953, 12, 0, 102, 27, 97...</td>\n",
       "      <td>[2036, 167, 82, 111, 6, 2705, 6355, 1, 114, 57...</td>\n",
       "      <td>[163, 164]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376583</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;doc id=\"376583\" url=\"https://th.wikipedia.org...</td>\n",
       "      <td>ลูนา 1 เป็นยานอวกาศลำแรกในโครงการลูนาของโซเวีย...</td>\n",
       "      <td>[139, 144]</td>\n",
       "      <td>เมชตา</td>\n",
       "      <td>[101, 43, 0, 99, 27, 26958, 12, 0, 102, 27, 97...</td>\n",
       "      <td>[1257, 188, 0, 62, 0, 6, 7727, 946, 82, 1, 487...</td>\n",
       "      <td>[55, 57]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  id                                            context  \\\n",
       "0      115035   1  <doc id=\"115035\" url=\"https://th.wikipedia.org...   \n",
       "1      376583   2  <doc id=\"376583\" url=\"https://th.wikipedia.org...   \n",
       "\n",
       "                                            question       label     answer  \\\n",
       "0  สุนัขตัวแรกรับบทเป็นเบนจี้ในภาพยนตร์เรื่อง Ben...  [529, 538]  ฮิกกิ้นส์   \n",
       "1  ลูนา 1 เป็นยานอวกาศลำแรกในโครงการลูนาของโซเวีย...  [139, 144]      เมชตา   \n",
       "\n",
       "                                         context_ids  \\\n",
       "0  [101, 43, 0, 99, 27, 26953, 12, 0, 102, 27, 97...   \n",
       "1  [101, 43, 0, 99, 27, 26958, 12, 0, 102, 27, 97...   \n",
       "\n",
       "                                        question_ids   label_idx  \n",
       "0  [2036, 167, 82, 111, 6, 2705, 6355, 1, 114, 57...  [163, 164]  \n",
       "1  [1257, 188, 0, 62, 0, 6, 7727, 946, 82, 1, 487...    [55, 57]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#label_idx is position in context_ids ,which value convert to word by idx2word[]\n",
    "label_idx = df_qa.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "df_qa['label_idx'] = label_idx\n",
    "print(f\"df_qa.shape:{df_qa.shape}\")\n",
    "display(df_qa.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "df3cb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(save_obj, path_file):\n",
    "    with open(path_file, 'wb') as file:\n",
    "        pickle.dump(save_obj, file)\n",
    "    print(f\"save {path_file[27:-4]} to {path_file} success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "665b7e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save df_qa to ./drqa/1-tokenizers/result/df_qa.pkl success\n",
      "save dict_word2idx to ./drqa/1-tokenizers/result/dict_word2idx.pkl success\n",
      "save dict_idx2word to ./drqa/1-tokenizers/result/dict_idx2word.pkl success\n",
      "save list_word_vocab to ./drqa/1-tokenizers/result/list_word_vocab.pkl success\n"
     ]
    }
   ],
   "source": [
    "#Save this State\n",
    "save_to_pickle(df_qa , \"./drqa/1-tokenizers/result/df_qa.pkl\")\n",
    "save_to_pickle(word2idx , \"./drqa/1-tokenizers/result/dict_word2idx.pkl\")\n",
    "save_to_pickle(idx2word , \"./drqa/1-tokenizers/result/dict_idx2word.pkl\")\n",
    "save_to_pickle(word_vocab , \"./drqa/1-tokenizers/result/list_word_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "40ae85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load object to use\n",
    "def load_pickle(path_file):\n",
    "    with open(path_file, 'rb') as file:\n",
    "        load_obj = pickle.load(file)\n",
    "        print(f\"load object from {path_file} success,that is {type(load_obj)}\")\n",
    "        return load_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "00018824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load object from ./drqa/1-tokenizers/result/dict_word2idx.pkl success,that is <class 'dict'>\n",
      "load object from ./drqa/1-tokenizers/result/dict_idx2word.pkl success,that is <class 'dict'>\n",
      "load object from ./drqa/1-tokenizers/result/list_word_vocab.pkl success,that is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "df_load = load_pickle(\"./drqa/1-tokenizers/result/df_qa.pkl\")\n",
    "word2idx_load = load_pickle(\"./drqa/1-tokenizers/result/dict_word2idx.pkl\")\n",
    "idx2word_load = load_pickle(\"./drqa/1-tokenizers/result/dict_idx2word.pkl\")\n",
    "vocab_load = load_pickle(\"./drqa/1-tokenizers/result/list_word_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "24d19d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load object from ./data/pretrained_wiki/thwiki_itos.pkl success,that is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "thwiki_itos = load_pickle(\"./data/pretrained_wiki/thwiki_itos.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "220020d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60005"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thwiki_itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf463b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_matrix():\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "    '''\n",
    "    glove_dict = {}\n",
    "    with open(\"./data//glove.840B.300d/glove.840B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = vector\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "29b1f958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4756/1264288433.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5bae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
